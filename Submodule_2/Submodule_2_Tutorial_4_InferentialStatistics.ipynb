{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5c8c48-8bae-4d86-ae10-8b83bc60b812",
   "metadata": {},
   "source": [
    "# Tutorial 4: Introduction to Inferential Statistics in Python\n",
    "------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1263f167-048a-46b7-ac35-11264a4e1704",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The purpose of performing **inferential statistics** in Python for bioinformatics is to make data-driven conclusions about biological phenomena, often from limited samples, while accounting for uncertainty. \n",
    " discoveries. Some parts of this tutorial will remind you of statistical analysis you should already know while introducing you to how the analysis can be performed with Python (Pandas, Numpy, and scikit-learn)\n",
    " \n",
    "## Learning objectives\n",
    "After this tutorial, you should be able to:\n",
    "- Carry out essential statistical methods, including ttests, ANOVAS, and correlations\n",
    "- Perform linear regression & query the OLS data\n",
    "- Compare model fits\n",
    "  \n",
    "## Prerequisites\n",
    "- You should be comfortable with Pandas and NumPy before using this tutorial\n",
    "- A background in biostatistics is necessary, as this tutorial generally will assume you know why you would want to do particular statistical analysis\n",
    "  \n",
    "## Getting started\n",
    "Please run the next code box to import the needed libraries, including\n",
    "- Numpy, Pandas, Matplotlib\n",
    "- Seaborn, statsmodels\n",
    "- Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d99eb-3883-49b6-9bd0-1c891945d6fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install seaborn\n",
    "%pip install statsmodels\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import sklearn\n",
    "print(\"All required packages have been installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef567e1d-323c-4744-a1d1-3d7877559377",
   "metadata": {},
   "source": [
    "# Why do inferential statisics in python?\n",
    "\n",
    "Bioinformatics typically involves large, complex datasets like gene expression levels, mutation rates, or protein interactions, and inferential statistics helps uncover patterns, relationships, and insights within this data. For example:\n",
    "\n",
    "1. **Hypothesis Testing**: Determine whether observed differences (e.g., gene expression under two conditions) are statistically significant.\n",
    "   - Example: Is a gene upregulated in diseased tissue compared to healthy tissue?\n",
    "   \n",
    "2. **Estimating Parameters**: Infer population characteristics, such as mutation frequencies or species diversity, from sample data.\n",
    "   - Example: Estimating the prevalence of a mutation in a population from sequencing data.\n",
    "\n",
    "3. **Correlations and Relationships**: Assess relationships between variables, such as gene expression levels and phenotypes, or identify co-expressed genes.\n",
    "   - Example: Is the expression of a gene correlated with disease severity?\n",
    "\n",
    "4. **Clustering and Classification**: Use statistical methods to group similar biological entities (e.g., clustering genes based on expression patterns) or classify data (e.g., distinguishing tumor subtypes).\n",
    "\n",
    "Python, with libraries like **statsmodels** and **scikit-learn**, provides robust tools for inferential statistics, making it ideal for analyzing bioinformatics datasets. These tools help researchers efficiently handle large datasets, automate workflows, and visualize results, enabling reproducible and insightful biological discoveries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c897e-7fa2-4834-9291-47b5f994a90d",
   "metadata": {},
   "source": [
    "## Our Working Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6311e0dc-6794-4882-b532-18ee3b5da651",
   "metadata": {},
   "source": [
    "Stats models comes with several datasets that can be loaded as a pandas Dataframe\n",
    "Load a few different pandas datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835a09b-504d-4ec3-8e2d-057f171ae9bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "## get datasets\n",
    "prot_exp=pd.read_csv(\".\" +os.sep + \"Datasets\" + os.sep + \"sim_prot_exp.csv\")\n",
    "mtcars = pd.read_csv(\".\" + os.sep + \"Datasets\" + os.sep + \"mtcars.csv\")\n",
    "mtcars[\"Transmission\"] = mtcars[\"am\"].replace({0: \"Automatic\", 1: \"Manual\"})\n",
    "mtcars.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b6888-fe82-4c19-9d6d-2c17690c20cf",
   "metadata": {},
   "source": [
    "### Describe the data\n",
    "We might be interested in comparing MPG’s across different automatic versus manual transmissions to test the hypothesis that **Manual transmission vehicles get better gas mileage than automatic.**\n",
    "We can first investigate the data on the 32 cars in the mtcars dataset. We'll use the convenient groupby object from Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9c097-5b38-4caa-b23d-dcd79171dce2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Comparing Two Distributions\n",
    "mtcars_group=mtcars.groupby(\"Transmission\")\n",
    "result=mtcars_group[\"mpg\"].agg([\"mean\",\"std\"])\n",
    "print(result)\n",
    "sns.boxplot(x=\"Transmission\", y=\"mpg\", data=mtcars);\n",
    "sns.stripplot(x=\"Transmission\", y=\"mpg\", data=mtcars, color=\"orange\", alpha=0.6, jitter=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d7e482-0ae5-405a-af30-e6b9250ca39c",
   "metadata": {},
   "source": [
    "The means and the medians are numerically different for the two transmission types, but the data all seems to overlap and there is not as much data for manual transmissions. This requires statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f47da2-2453-4afd-9387-cb6afe1b827b",
   "metadata": {},
   "source": [
    "## Essential Statistical Methods for Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b9cbf-1c56-410d-b1f5-e699e2152b44",
   "metadata": {},
   "source": [
    "### Comparing Normal Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd75327-df7a-40af-9bac-41af12d926f2",
   "metadata": {},
   "source": [
    "If we ask the question, \"Do the miles per gallon (mpgs) differ by transmission (trans)?\", what do we mean statistically by this? \n",
    "\n",
    "To start answering this question, how would you describe the distribution of mpgs, independent of transmission type? (in code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca85a4f5-8e29-4087-8bd2-ffad89c40134",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Sufficient Statistics for the normal distribution\n",
    "cars_mpg_mean=mtcars.mpg.mean()\n",
    "cars_mpg_var= mtcars.mpg.var()\n",
    "print(\"The overall mpg mean is \" + str(cars_mpg_mean.round(2)) + \" with variance \" + str(round(cars_mpg_var,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a0214-ea91-483e-bc45-4563c4601a8f",
   "metadata": {},
   "source": [
    "Perhaps you have heard of the \"central limit theorem (CLT)?\" This theorem says that no matter the underlying distribution of some values, if we take a sample and then calculate the sample average, the distribution of those sample averages will be **normal.**\n",
    "<br>So, how do we estimate the underlying population mean?\n",
    "We start by calculating the sample average and the sample variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4813be-84bb-4ca0-8b7d-e9303324b71c",
   "metadata": {},
   "source": [
    "### Comparing Two Distributions\n",
    "When we compare two distributions, we are really comparing the sample average distributions..\n",
    "Then, what does it mean to conclude two distributions are different?\n",
    "That means we believe their underlying population means are different!\n",
    "“Sure they are different, but are they statistically significantly different? “"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffb6537-1b3c-471d-95ac-0cec85a2a943",
   "metadata": {},
   "source": [
    "We can visually compare the two different distributions of data.\n",
    "We might start to ask ourselves if they **look that different** and if we have **normally distributed data**. \n",
    "Remember, the bigger the difference OR the more observations I have, the more certain I am of the result…\n",
    "\n",
    "First, a histogram of the data to look for the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bfb90d-d30e-434a-903b-f0ad5cf0332e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Compare MPGs across groups\n",
    "#mtcars_group.mpg.hist();\n",
    "colors = {\"Manual\": \"blue\", \"Automatic\": \"orange\"}  # Define colors for groups\n",
    "for name, group in mtcars_group:\n",
    "    plt.hist(group[\"mpg\"], bins=5, alpha=0.5, label=name, color=colors[name])\n",
    "plt.xlabel(\"Miles per gallon\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bbe7c7-dc95-43d0-bc97-adc3d183313c",
   "metadata": {},
   "source": [
    "The two groups have relatively small numbers, so it is hard to see a bell-shaped curve, especially for the manual transmission cars.\n",
    "\n",
    "It does APPEAR that the means are different, but how can we feel confident?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e0618-3673-40b0-bb71-c65f3b74a5f4",
   "metadata": {},
   "source": [
    "### P-Values\n",
    "\n",
    "Imagine you are flipping a coin. Will the next flip be heads or tails?\n",
    "For one flip, it is easy: there is a 50% probability for each. But what if the\n",
    "next three flips were all heads? The next 10? The next 50?\n",
    "Would you still think there was an equal possibility for a head or tail \n",
    "on the next flip or would you conclude (infer) the coin was not actually fair?\n",
    "\n",
    "Here's where the p-value comes in. The p-value gives us a threshold by which we make a\n",
    "determination whether a given hypothesis is valid or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540bb1f1-17ad-44a7-9a7c-f0597f7c7799",
   "metadata": {},
   "source": [
    "You could define a p-value this way:\n",
    "\n",
    "“Given some hypothesis, what is the probability of getting this result (or one more extreme) assuming the null (or opposite) hypothesis is true?”\n",
    "\n",
    "In other words, suppose you believe the coin in the last example is \"not fair\". What is the probability, given the null\n",
    "hypothesis (the coin is \"fair\") that you would get 10 heads in a row?\n",
    "You can calculate the p-value as 1/2^10 or 1/1024 or less than .1%. This means there is less than a .1% chance a fair coin will give you ten heads in a row.\n",
    "\n",
    "What does that mean in this case. If you set your threshold of 5% and your p-value is less (in this case it is) than the null hypothesis is \"false\", meaning you can infer the coin that the opposite is true or that the coin is NOT fair. This is how inferential statistics works.\n",
    "Now let's see how that applies to comparing two distributions.\n",
    "Hypothesis: The distributions are different.\n",
    "Null hypothesis: The distribtions are the same.\n",
    "\n",
    "<mark>We can  use statistical tests such as the t-test or the Welch's t-test to generate\n",
    "a p-value for comparison.</mark>\n",
    "\n",
    "We'll import a tool for ttest from the scipy.stats library\n",
    "\n",
    "Our standard will be to NOT REJECT the hypothesis if p<0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf238d-23ec-4380-907e-9ecdbc6849e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "# Extract groups\n",
    "autoT = mtcars[mtcars[\"Transmission\"] == \"Automatic\"][\"mpg\"]\n",
    "manT = mtcars[mtcars[\"Transmission\"] == \"Manual\"][\"mpg\"]\n",
    "\n",
    "# Perform independent two-sample t-test\n",
    "t_stat, p_value = ttest_ind(autoT, manT)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f7e85-ee59-45c6-8bd2-29338ba3a4b7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Success:</b>The p-value was less than 0.01, so we conclude that the gas mileage is higher for manual transmission.</a>. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7db6484-8b9d-422c-ac32-4dff45f6f45e",
   "metadata": {},
   "source": [
    "### Test your knowledge\n",
    "Now it's your turn to repeat this analysis for different data. \n",
    "We used the protein expression dataframe in the Visualizing Data module (prot_exp) which we already loaded \n",
    "\n",
    "1. Groupby \"group\" (control & treated)\n",
    "2. Create a histogram of the data\n",
    "3. Measure the mean & variance\n",
    "4. Create a boxplox of the two groups (Treated, Control)\n",
    "5. Perform a ttest & determine the probability that the two samples come from different distributions (the p-value) to answer the quiz question.\n",
    "\n",
    "*The solution is at the end of this tutorial.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a869143-7a82-4768-b63a-1fd782120381",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#TTest task on prot_exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71bf54-388e-47fd-a54a-a4a736675e6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "ttest_prot= \"PythonQuizQuestions/prot_ttest.json\"\n",
    "display_quiz(ttest_prot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97921f8a-a72c-48ed-a85d-84cd4b3eacc1",
   "metadata": {},
   "source": [
    "### ANOVA\n",
    "What if there are more than two distributions to test? For that we can use a more\n",
    "sophisticated test such as ANOVA (assuming normal distribution) or the Kruskal-Wallis \n",
    "test (for medians).\n",
    "\n",
    "We now are asking the questions:\n",
    "1. are these population means unequal?\n",
    "2. Is at least one of the group means not equal to the others?\n",
    "\n",
    "The formal statistical test for this comparison is an **ANOVA**\n",
    "\n",
    "The mtcars dataframe has representative cars with different numbers of cylinders (cyl). We can ask whether the mpg differs between those groups with an ANOVA.\n",
    "\n",
    "There are many Python tools for performing ANOVA. We'll show 2 different approaches. \n",
    "\n",
    "First, we'll manually break the data into groups based on the number of cylinders then show how statsmodel can determine the groups & analyze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6092bd-cae6-4263-9065-7bcacf174ebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#What categories are there for the number of cylinders in the cars?\n",
    "mtcars[\"cyl\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c107e-a9ac-4cb9-83bd-d278aca2f2bb",
   "metadata": {},
   "source": [
    "We know what our categories are, so we can break the data into groups based on the number of cylinders & use scipy's library for calculating the one-way F-statistic (technically the value used to perform the ANOVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf3c2f7-9048-416e-85fe-efa4aae0364a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Scipy's approach requires that you know the identities of your groups\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Split the data into groups\n",
    "group_4 = mtcars[mtcars[\"cyl\"] == 4][\"mpg\"]\n",
    "group_6 = mtcars[mtcars[\"cyl\"] == 6][\"mpg\"]\n",
    "group_8 = mtcars[mtcars[\"cyl\"] == 8][\"mpg\"]\n",
    "\n",
    "# Perform ANOVA\n",
    "f_stat, p_value = f_oneway(group_4, group_6, group_8)\n",
    "\n",
    "print(f\"F-statistic: {f_stat:.2f}, p-value: {p_value:.4f}\") #.2f and .4f tell how many decimals to provide, 2 or 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4b1cf0-2bac-4447-995f-14f115932edc",
   "metadata": {},
   "source": [
    "Rather than having to create our own groups to compare, since that is tedious, we can let statsmodel do the work. \n",
    "\n",
    "Run the code below then read the explanation after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a1fd6-2d9e-4086-aed2-391ce947df45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Create an OLS model\n",
    "model = ols(\"mpg  ~ C(cyl)\", data=mtcars).fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac83e2-d1ad-4ce0-b40d-4f91226661b3",
   "metadata": {},
   "source": [
    "We ask python to perform an ordinary least squares (OLS) fit of the mtcars data, comparing mpg to the number of cylinders.\n",
    "\n",
    "By writing C(cyl) we tell the tool that it should treat cyl not as a continuous variable (the default) but as a category.\n",
    "\n",
    "The C(cyl) indicates that the variable cyl (number of cylinders) is treated as categorical.\n",
    "Without C(), cyl would be treated as a continuous numerical variable.\n",
    "By using C(cyl), you're instructing the model to group cyl into categories (e.g., 4 cylinders, 6 cylinders, 8 cylinders).\n",
    "\n",
    "Now, run the next code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875cfc1d-e242-4d08-9ccb-d13f8aad9b1b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Perform ANOVA\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d529b85-8d74-4346-a3be-f95f7ed4d50b",
   "metadata": {},
   "source": [
    "The statsmodel tool:\n",
    "- Performs the ANOVA on the fitted model.\n",
    "- Type II ANOVA calculates the sum of squares for each variable adjusted for others.\n",
    "- The output (anova_table) provides the statistical evidence (F-statistic and p-value) to test whether the independent variable(s) significantly affect the dependent variable.\n",
    "\n",
    "In this case, as with the previous ANOVA using f_oneway, we see that **the mpg IS affected by the number of cylinders (p<0.01)** in this case giving us the value: 5e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6e918-9e9f-4a56-820d-7bce29abd966",
   "metadata": {},
   "source": [
    "### Test your knowledge\n",
    "You are ready to perform an ANOVA on a different dataset (mut_rate.csv) in the Datasets folder\n",
    "\n",
    "Context: The numbers are mutation rates (e.g., mutations per 100,000 bases) for genomes sequenced from different groups. For example:\n",
    "- Group_A: Mutation rates in normal tissue samples.\n",
    "- Group_B: Mutation rates in tumor tissue.\n",
    "- Group_C: Mutation rates in metastatic tissue.\n",
    "\n",
    "You can use ANOVA to test the following hypotheses:\n",
    "- Null Hypothesis: The mean mutation rates are equal across all groups (e.g., normal tissue, tumor tissue, and metastatic tissue).\n",
    "- Alternative Hypothesis At least one group's mean mutation rate is significantly different from the others.\n",
    "\n",
    "A solution is at the end of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897bc350-506a-4eb4-aa59-613ced951b87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#ANOVA for mut_rate.csv\n",
    "#load into a Pandas dataframe\n",
    "import os\n",
    "#examine the first few lines to be sure of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f78ee-363b-4739-9705-2060829a32e6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "mut_anova_qz=\"PythonQuizQuestions/mut_anova.json\"\n",
    "display_quiz(mut_anova_qz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40ca191-eb17-43ff-baf5-03360e3e756f",
   "metadata": {},
   "source": [
    "## Comparing Continuous distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66638767-9f7f-4484-933e-1426139f5a50",
   "metadata": {},
   "source": [
    "We just compared continuous distributions with *categorical* distributions\n",
    "\n",
    "What about a continuous distribution with a *continuous* distribution?\n",
    "\n",
    "We typically summarize continuous-by-continuous relationships as a **correlation.**\n",
    "\n",
    "Simply put, do these variables move together? Is there a relationship between them? \n",
    "\n",
    "Seaborn libraries can help us to see these relationships in mtcars.\n",
    "\n",
    "Run the next code box to show the full data pattern for the relationship between engine displacement (disp) or horsepower (hp), with the two histograms on the x & y axis. \n",
    "\n",
    "The scatterplot shows the x-y data and the correlation line (solid) with the confidence interval of the line in light blue. \n",
    "\n",
    "(Think about how long it would take to get Excel to make even part of these plots!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33a289-3674-445b-8265-0d35e1d70401",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Correlation\n",
    "## seaborn image \n",
    "sns.jointplot(x=\"mpg\", y=\"disp\", data=mtcars, kind=\"reg\")\n",
    "sns.jointplot(x=\"mpg\", y=\"hp\", data=mtcars, kind=\"reg\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba249e-71ce-4e2d-863a-4f97c79cbae2",
   "metadata": {},
   "source": [
    "We observe that the fit to the displacement~mpg may be slightly better than that of horsepower, but this should be evaluated quantitatively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9cdba-1da7-41ca-ad90-420d8a03a064",
   "metadata": {},
   "source": [
    "### Calculate Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e4f56-d2ab-4530-9464-b768b4ca3c6e",
   "metadata": {},
   "source": [
    "We can calculate correlation as shown below. The default method is linear correlation (Pearson)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d325f-be60-45d0-9c5d-f65c190089b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Examine the linear correlation between mpg and other values\n",
    "mtcars.loc[:,[\"mpg\",\"disp\",\"hp\"]].corr()\n",
    "\n",
    "## Calculate the covariance\n",
    "#mtcars.loc[:,[\"mpg\",\"disp\",\"hp\"]].cov()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10877ff3-b6da-43ac-ad4b-213111762ca8",
   "metadata": {},
   "source": [
    "#### Meeting assumptions?\n",
    "Python is willing to perform statistical tests on whatever data you feed it. If you ask commercial stats software (e.g., SigmaPlot) to do this, it first runs normality test on the data. This is because it is illegitimate to perform Pearson correlations which assumes normal distributions. If normality is violated, one can use Spearman correlation, which does not assume normality.\n",
    "\n",
    "Scipy provides the tools for the Shapiro-Wilk test of normality. It's so easy to run that we have no excuse for ignoring this possible violation. \n",
    "\n",
    "The next box shows the Shapiro-Wilk test on these 3 parameters in mtcars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001538f4-a289-4bbe-a970-ff6035ea0e4c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "# Check normality of 'hp'\n",
    "stat_hp, p_hp = shapiro(mtcars['hp'])\n",
    "print(f\"Shapiro-Wilk test for 'hp': p-value = {p_hp:.4f}\")\n",
    "\n",
    "# Check normality of 'disp'\n",
    "stat_disp, p_disp = shapiro(mtcars['disp'])\n",
    "print(f\"Shapiro-Wilk test for 'disp': p-value = {p_disp:.4f}\")\n",
    "\n",
    "# Check normality of 'mpg'\n",
    "stat_mpg, p_mpg = shapiro(mtcars['mpg'])\n",
    "print(f\"Shapiro-Wilk test for 'mpg': p-value = {p_mpg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5990f-065a-48ad-b748-c53162b315cf",
   "metadata": {},
   "source": [
    "The mpg IS normal (accept null hypothesis since p>0.05) but, not surprisingly, the displacement and horsepower are NOT normal (after all, they are selected by manufacturers.)\n",
    "\n",
    "We should therefore not do the default Pearson correlation but rather the Spearman version. This is a simple switch to tell the correlation function which method to use.\n",
    "\n",
    "The correlation coefficients are about 0.1 units different from Pearson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e3445-cd0b-4411-a712-f870e00203bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Perform a Spearman correlation on the mtcars data\n",
    "mtcars.loc[:,[\"mpg\",\"disp\",\"hp\"]].corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0c126-6a37-4c5e-8aa5-524fc580423f",
   "metadata": {},
   "source": [
    "Now, it's your turn. Use the provided dataset with three variables: Gene_Expression, Protein_Abundance & Disease_Severity. Assess the normality then, using either Pearson or Spearman, determine the correlations. \n",
    "\n",
    "Then, check your answers with the quiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f51aa-9d37-4ef8-85f3-e7e6cf753282",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "import os\n",
    "disease= pd.read_csv(\".\"+ os.sep+ \"Datasets\" +os.sep + \"bioinform_disease.csv\")\n",
    "\n",
    "# Check normality\n",
    "\n",
    "# Calculate correlations between the 3 variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a0ea12-3a3c-466c-ad51-811d2634fa4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "dis_cor= \"PythonQuizQuestions/disease_corr_qz.json\"\n",
    "display_quiz(dis_cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ecaff-9e36-4c37-82ec-d3282ee5c605",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "We will focus, for now, on regular linear regression for multivariate statistics, but all lessons learned apply to other methods. \n",
    "\n",
    "What are the assumptions of linear regression?\n",
    "- Independence\n",
    "- Normality\n",
    "- Linearity\n",
    "- Homoscedasticity\n",
    "\n",
    "Y ~ B0 + B1 * X1 + B2*X2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c153262e-e2da-458d-8b67-98dfdd1f8b5a",
   "metadata": {},
   "source": [
    "Linear Regression is the heart of modern frequentist statistics\n",
    "Explore relationships between predictors and an outcome\n",
    "There are different types of regression (see below).\n",
    "Use regression to answer certain questions, but also understand your inputs.\n",
    "  What type of data?\n",
    "  What size of data?\n",
    "  What are your intended outputs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7230c4f-2bc5-42f0-8359-aad8b1ebeee6",
   "metadata": {},
   "source": [
    "### Regression Types in this tutorial\n",
    "While there are many possible relationships we might want to evaluate between continuous or categorical variables, this tutorial will show you the basics of: \n",
    "\n",
    "- OLS Regression, or simple linear regression, in forms we've already seen (partially)\n",
    "\n",
    "- Multiple Linear Regression\n",
    "    As a predictive analysis, multiple linear regression is used to explain the relationship between one \n",
    "    continuous dependent variable and two or more independent variables.\n",
    "\n",
    "= Polynomial regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e1505-b7c5-4531-b169-7b10c7870aa8",
   "metadata": {},
   "source": [
    "### The General Regression Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f3137b-bba9-4c27-9e36-a2a0c4a7e085",
   "metadata": {},
   "source": [
    "Regression analysis requires a series of steps:\n",
    "1. Collect appropriate data\n",
    "2. Identify Outcomes / predictors\n",
    "3. Investigate \"shape\" of data\n",
    "4. Consider appropriate functional form\n",
    "5. Apply prior knowledge\n",
    "6. Select \"best\" model and variables\n",
    "7. Check model for validity (residuals, etc)\n",
    "8. Apply model\n",
    "9. Predict new results\n",
    "\n",
    "Python tools can be used for the last 4 which we will demonstrate below. Again, the assumption is that the analytical technique is being demonstrated rather than teaching the foundational statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f1294-58d7-41e4-8569-6f401f787c79",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares (OLS) Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a9873-e1da-46d3-ac63-6e149e1467f7",
   "metadata": {},
   "source": [
    "Ordinary Least Squares is a method used in regression analysis to estimate the coefficients of a <u>linear</u> model by minimizing the sum of squared residuals (the squared differences between the observed and predicted values). This method ensures that the regression line (or hyperplane, in the case of multiple regression) best fits the data.\n",
    "\n",
    "It is easy to learn conceptually and you are probably familiar with this process. \n",
    "\n",
    "In bioinformatics, we use OLS for many things, including:\n",
    "- **Gene Expression Analysis:** To identify relationships between gene expression levels and experimental conditions or phenotypes.\n",
    "- **Genome-Wide Association Studies (GWAS):** To model associations between genetic variants (e.g., SNPs) and traits of interest.\n",
    "- **Protein-Protein Interaction Modeling:** To predict interactions based on quantitative features like binding affinities or structural properties.\n",
    "- **Medication Dosage vs. Treatment Effect:** To determine the relationship between drug dose and improvement in patient symptoms.\n",
    "\n",
    "For OLS regression, we have to make some assumptions (this is key): Normality, Independence, Linearity, Homoscedasticity  *but we won't discuss these here- rather, just how to make the calculations*\n",
    "\n",
    "Formula: y = xb + e or y = b0 + b1x + e\n",
    "- x – some input\n",
    "- b – some vector of parameters\n",
    "- e – some error\n",
    "- B0 – constant/y-intercept\n",
    "- B1 - slope\n",
    "\n",
    "We are regressing <u>engine displacement</u> on <u>mpg</u>, two continuous variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee4b73-c26d-419e-b9ae-17f0a35e91c7",
   "metadata": {},
   "source": [
    "We have to give a starting \"constant\" value to statsmodel (sm) in order to keep it from forcing a fit through the origin (0,0). The actual B0 will then be calculated. \n",
    "\n",
    "Try to write out the regression equation that is being fit below. You are looking for b0, b1, assume e = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb058df8-373e-4907-aa87-810365ab7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import statsmodel as sm\n",
    "#mtcars=pd.read_csv(\"./Datasets/mtcars.csv\")\n",
    "\n",
    "## Simple Linear Regression (One X predictor)\n",
    "## Fit regression model\n",
    "mtcars[\"constant\"] = 1   #makes a new column with the constant value\n",
    "X = mtcars.loc[:,[\"constant\",\"disp\"]]              #creates a new dataframe, X, with all the rows :, \n",
    "                                                   #and the two columns: constant & disp\n",
    "Y = mtcars.mpg\n",
    "model1res = sm.OLS(Y, X).fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c833f-df59-4ed9-b6b4-322733a6543c",
   "metadata": {},
   "source": [
    "### Model Summary Results\n",
    "\n",
    "Use Python to inspect the 'OLS results object' with summary, with two different format functions (summary1 & summary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "714efe29-1c9e-4c64-87b5-70702cb73700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.718\n",
      "Model:                            OLS   Adj. R-squared:                  0.709\n",
      "Method:                 Least Squares   F-statistic:                     76.51\n",
      "Date:                Sat, 29 Mar 2025   Prob (F-statistic):           9.38e-10\n",
      "Time:                        10:59:10   Log-Likelihood:                -82.105\n",
      "No. Observations:                  32   AIC:                             168.2\n",
      "Df Residuals:                      30   BIC:                             171.1\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "constant      29.5999      1.230     24.070      0.000      27.088      32.111\n",
      "disp          -0.0412      0.005     -8.747      0.000      -0.051      -0.032\n",
      "==============================================================================\n",
      "Omnibus:                        3.368   Durbin-Watson:                   1.250\n",
      "Prob(Omnibus):                  0.186   Jarque-Bera (JB):                3.049\n",
      "Skew:                           0.719   Prob(JB):                        0.218\n",
      "Kurtosis:                       2.532   Cond. No.                         558.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "Version summary2 below\n",
      "                 Results: Ordinary least squares\n",
      "=================================================================\n",
      "Model:              OLS              Adj. R-squared:     0.709   \n",
      "Dependent Variable: mpg              AIC:                168.2094\n",
      "Date:               2025-03-29 10:59 BIC:                171.1409\n",
      "No. Observations:   32               Log-Likelihood:     -82.105 \n",
      "Df Model:           1                F-statistic:        76.51   \n",
      "Df Residuals:       30               Prob (F-statistic): 9.38e-10\n",
      "R-squared:          0.718            Scale:              10.572  \n",
      "------------------------------------------------------------------\n",
      "               Coef.   Std.Err.     t     P>|t|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "constant      29.5999    1.2297  24.0704  0.0000  27.0884  32.1113\n",
      "disp          -0.0412    0.0047  -8.7472  0.0000  -0.0508  -0.0316\n",
      "-----------------------------------------------------------------\n",
      "Omnibus:              3.368        Durbin-Watson:           1.250\n",
      "Prob(Omnibus):        0.186        Jarque-Bera (JB):        3.049\n",
      "Skew:                 0.719        Prob(JB):                0.218\n",
      "Kurtosis:             2.532        Condition No.:           558  \n",
      "=================================================================\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the\n",
      "errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "## Inspect the results\n",
    "print(model1res.summary())\n",
    "print(\"\\nVersion summary2 below\")\n",
    "print(model1res.summary2()) ## different format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad4be9-0140-458f-bcaf-8e954b1a6135",
   "metadata": {},
   "source": [
    "\n",
    "That output can be overwhelming. \n",
    "\n",
    "Let's examine the model for some *specific* traits by calling out particular values.\n",
    "    Use dir(model1res) to see all the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde8841-6d07-442b-9ad3-61cf4e3742c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get AIC for model\n",
    "print(\"AIC: \" + str(model1res.aic))\n",
    "## Get r-squared\n",
    "print(\"rsq: \" + str(model1res.rsquared))\n",
    "## r-squared penalized for the number of parameters\n",
    "print(\"mod rsq: \" + str(model1res.rsquared_adj))\n",
    "\n",
    "## Mean squared error...?\n",
    "1 - model1res.mse_resid/model1res.mse_total\n",
    "print(\"mse: \" + str(model1res.mse_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32043f51-1d0a-4475-a0be-fdc90b855e63",
   "metadata": {},
   "source": [
    "We can also pull out the **residuals.**\n",
    "\n",
    "Residuals in regression analysis are the differences between the observed values and the predicted values produced by the regression model. They represent the part of the data that the model could not explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeaefcc-2b42-4b03-99d0-984ff0ede568",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## View residuals\n",
    "model1res.resid\n",
    "model1res.resid_pearson\n",
    "## View Predicted Values\n",
    "model1res.predict()\n",
    "\n",
    "## Get the influence of each value? DBETAS?\n",
    "model1res.get_influence()\n",
    "\n",
    "## View available model attributes\n",
    "dir(model1res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c13ea-5335-49c7-9881-e187e7290cd1",
   "metadata": {},
   "source": [
    "We can request some automatic plots using stats models graphics. You will be able to see where most of the residuals (the weakest fit) exists in the system, and how well the fitted data seems to encapsulate the relationship shown in the data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06592eb-a67d-420a-8f29-cae960711c3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Goodness of fit\n",
    "## Find outlier observations\n",
    "model1res.outlier_test()\n",
    "\n",
    "## Goodness of fit plots\n",
    "sm.graphics.plot_regress_exog(model1res, \"disp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80580386-562b-4aa3-8987-8a4ab2ba2cf6",
   "metadata": {},
   "source": [
    "### Regression with Categorical Variables\n",
    "The goal of regression is to model the linear relationship between the dependent variable and the predictors (even if some predictors are categorical).\n",
    "\n",
    "Linear regression can handle categorical variables **if** they are properly encoded into numerical form. \n",
    "\n",
    "In mtcars, the dependent variable (Y, mpg) is continuous.\n",
    "\n",
    "The independent variable (X) am (transmission type) is categorical (0 = automatic, 1 = manual).\n",
    "The change in mpg when switching from automatic (am = 0) to manual (am = 1).\n",
    "\n",
    "This regression gives you insights into how am (a categorical variable) affects mpg (a continuous variable). For example:\n",
    "\n",
    "The number of cylinders is written as a string. We have several ways of handling this: one-hot coding tool from Numpy, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb62b5d9-4704-4503-9c3f-917dd1295081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.360\n",
      "Model:                            OLS   Adj. R-squared:                  0.338\n",
      "Method:                 Least Squares   F-statistic:                     16.86\n",
      "Date:                Sat, 29 Mar 2025   Prob (F-statistic):           0.000285\n",
      "Time:                        12:11:28   Log-Likelihood:                -95.242\n",
      "No. Observations:                  32   AIC:                             194.5\n",
      "Df Residuals:                      30   BIC:                             197.4\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "constant      17.1474      1.125     15.247      0.000      14.851      19.444\n",
      "am             7.2449      1.764      4.106      0.000       3.642      10.848\n",
      "==============================================================================\n",
      "Omnibus:                        0.480   Durbin-Watson:                   1.065\n",
      "Prob(Omnibus):                  0.787   Jarque-Bera (JB):                0.589\n",
      "Skew:                           0.051   Prob(JB):                        0.745\n",
      "Kurtosis:                       2.343   Cond. No.                         2.46\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "## Linear Regression with Categorical predictors\n",
    "## mpg and transmission type\n",
    "## Fit regression model\n",
    "mtcars[\"constant\"] = 1\n",
    "X = mtcars.loc[:,[\"constant\",\"am\"]]\n",
    "Y = mtcars.mpg\n",
    "mod2res = sm.OLS(Y, X).fit()\n",
    "print(mod2res.summary())\n",
    "## Alternatively:\n",
    "#X = sm.add_constant(mtcars.am)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01272e53-5e2d-4f64-9d9b-a7a590ddce78",
   "metadata": {},
   "source": [
    "### Dummy Coding Variables\n",
    "The cylinder value in mtcars has values of 4, 6, and 8. We need to code these values into new variables so they are written as 0 & 1.\n",
    "\n",
    "You can fix this by adding a dummy variable. (\"One hot coding\" as we learned in the [Numpy tutorial](./Submodule_2_Tutorial1_NumPy.ipynb) would also have the same effect) \n",
    "\n",
    "Essentially, we will make one new variable in the dataframe for each of the values of the cyl array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9fabc7-758c-4d51-8a85-340519e38a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need to dummy code the categorical variable\n",
    "mtcars['cyl_4']=0           #by default, all values in the new variable 'cyl_4' will be zero\n",
    "mtcars.loc[mtcars.cyl== 4, 'cycl_4']=1       #replace the \n",
    "mtcars['cyl_6'] = 0\n",
    "mtcars.loc[mtcars.cyl == 6,'cyl_6'] = 1\n",
    "mtcars['cyl_8'] = 0\n",
    "mtcars.loc[mtcars.cyl == 8,'cyl_8'] = 1\n",
    "## Rerun the model with more parameters\n",
    "X = mtcars.loc[:,[\"constant\",\"cyl_6\",\"cyl_8\"]]\n",
    "Y = mtcars.mpg\n",
    "mod3res = sm.OLS(Y, X).fit()\n",
    "print(mod3res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955e2bd-66b1-46aa-b997-74c1a2fa161b",
   "metadata": {},
   "source": [
    "Is there an easier way to set up these matrices?!?\n",
    "\n",
    "We can use the **formula interface**. We can tell statsmodel to treat the numbers in the cyl column as if they are categories: C(cyl).\n",
    "\n",
    "(We also can one-hot encode with numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48f5be-b41c-4c34-9e24-330edbbc9836",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Formula interface included with statsmodels\n",
    "## Fit same model\n",
    "mod4res = smf.ols('mpg ~ C(cyl)', data=mtcars).fit()\n",
    "mod4res.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c2003-03c7-41d5-8a2f-55d51d01ca4f",
   "metadata": {},
   "source": [
    "### Test Your Knowledge- regression with categorical variables\n",
    "This example on car gas mileage can be applied to our mutation rate dataset.\n",
    "\n",
    "Goal: Fit a linear regression model to estimate how the Group (categorical) affects Values (continuous). \n",
    "\n",
    "(You may want to  make the mutrate dataframe again from \"./Datasets/mut_rate.csv\")\n",
    "\n",
    "The steps would be:\n",
    "1. Make a new dataframe that has only the normal & metastatic data (Groups A & C)\n",
    "2. Dummy code the normal (Group A) and metastatic (Group B) groups as 0 and 1 by adding a column\n",
    "3. Set up the regression (X & Y variables)\n",
    "(sample code below, after the quiz)\n",
    "*The quiz includes an additional question on Group A vs. Group B (normal vs. tumor, rather than metastatic)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c421b-2e11-4476-9f22-ea4a5e8eccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your own dummy-coded Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f2683-7adb-43ca-9723-3389c5208a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "mut_ols_quiz=\"./PythonQuizQuestions/mut_ols.json\"\n",
    "display_quiz(mut_ols_quiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f786ad-f0c7-47bf-ae68-144aea1f12e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Example code\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load data\n",
    "mutrate = pd.read_csv(\"./Datasets/mut_rate.csv\")\n",
    "mutAC = mutrate.loc[mutrate[\"Group\"] != \"Group_B\",:].copy()    #Exludes group b. Using a copy keeps pandas from giving a warning.\n",
    "                                                               #which you can see if you just remove the .copy().\n",
    "                                                               #A clean copy of the original makes the behavior more predictable\n",
    "mutAC[\"Metastatic\"]=(mutAC[\"Group\"]== \"Group_C\").astype(int)    #makes True or False into a number\n",
    "mutAC[\"constant\"] = 1\n",
    "X = mutAC.loc[:,[\"constant\",\"Metastatic\"]]\n",
    "Y = mutAC.Values\n",
    "mut_model = sm.OLS(Y, X).fit()\n",
    "print(mut_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0034c-f979-4dc3-bc43-116b5ecd455f",
   "metadata": {},
   "source": [
    "### Multiple Regression (Linear)\n",
    "\n",
    "Multiple Linear Regression is a statistical method used to understand and predict the relationship between a single dependent variable (the outcome) and two or more independent variables (the predictors). It allows you to assess how each predictor contributes to the outcome while considering the influence of other predictors.\n",
    "\n",
    "With Statsmodel.formula, we write this as: \n",
    "\n",
    "model = smf.ols(dependent_variable ~ Ind1 + Ind2 + ... + IndN).fit()\n",
    "\n",
    "where Ind1 etc are the independent variables. They can be categorical (e.g., C(cyl) or \"am\" already dummy coded)\n",
    "\n",
    "A key question with multiple regression is **\"Are we able to better explain the variation in the dependent variable when we consider the effect of more and more independent variables?\"**\n",
    "\n",
    "The tools for model fit comparison are described in the next section.  **BUT** look below at the Interactions section to see how to obtain different insights for multiple predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7972201-b195-4b4a-aca5-54a01e2c2486",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Linear regression with multiple predictors\n",
    "\n",
    "mod5res = smf.ols('mpg ~ hp + disp', data=mtcars).fit()\n",
    "mod5res.summary() \n",
    "\n",
    "mod6res = smf.ols('mpg ~ C(cyl) + disp + am', data=mtcars).fit()  #\n",
    "mod6res.summary() \n",
    "\n",
    "## Does AM add anything?\n",
    "sm.stats.anova_lm(mod6res, typ=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96e497c-4e87-46f9-8556-69d3c24198b6",
   "metadata": {},
   "source": [
    "#### Tools for chosing the best multiple regression model\n",
    "\n",
    "We compare model fits using metrics like AIC (Akaike Information Criterion) and Sum of Squared Errors (SSE) to evaluate how well different models explain the data and to avoid **overfitting or underfitting.**\n",
    "\n",
    "Why Compare Model Fits?\n",
    "Model Quality: Some models might fit the data better than others, but a good fit doesn’t always mean the model is the best. Comparing fits helps identify the model that balances accuracy and complexity.\n",
    "\n",
    "Overfitting vs. Simplicity:\n",
    "\n",
    "- AIC penalizes models with too many parameters to prevent overfitting, helping you choose a simpler, generalizable model.\n",
    "- Likelihood ratio tests which tests to see that a more complex model is better at fitting than the same model with fewer parameters.\n",
    "- SSE measures how closely the model’s predictions match the observed data. Lower values mean better fit, but SSE alone doesn’t account for model complexity.\n",
    "\n",
    "\n",
    "Selection of the Best Model: Comparing metrics ensures you pick the most appropriate model for prediction or inference, especially when you have multiple competing models with different variables or structures.\n",
    "\n",
    "Because these are so crucial, these parameters are automatically calculated in the OLS (see charts above). Here we just print them from the model parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a341177-430f-4a09-be61-62c958ef39f9",
   "metadata": {},
   "source": [
    "#### AIC\n",
    "We can compare model fits with **AIC** to \n",
    "- a likelihood-based statistic penalized for the number of parameters\n",
    "- the smaller the value, the better the fit\n",
    "- AIC values within 2 of each other are considered equivalent.\n",
    "\n",
    "You see that the result of the model fits is that model 4 is worst, but the other 2 are essentially equally good at fitting the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c949ac7-fedd-486b-96d0-fce487ff165a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## comparing model fits - AIC\n",
    "print(\"model-6 AIC: \", mod6res.aic)\n",
    "print(\"model-5 AIC: \", mod5res.aic)\n",
    "print(\"model-4 AIC: \", mod4res.aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a95f2-897d-4cd9-9fac-b0e0bf8f0d4a",
   "metadata": {},
   "source": [
    "#### Likelihood ratio test\n",
    "We can use a LRT for models that are “nested”\n",
    "Meaning one model can be recovered from the other by deleting parameters…\n",
    "If they are not nested, we must use AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e90310b-9633-473c-b892-bd04e9f7c01e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Compare Model fits - likelihood\n",
    "print(\"model-6 LLF: \", mod6res.llf)\n",
    "print(\"model-5 LLF: \", mod5res.llf)\n",
    "\n",
    "## Likelihood ratio test\n",
    "sm.stats.anova_lm(mod5res,mod6res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc7450-2e7b-4722-93df-af3cac162918",
   "metadata": {},
   "source": [
    "#### Sum of Squared Errors\n",
    "We can also use R-squared, which can be interpreted as the proportion of variance explained by the model. \n",
    "If r-squared goes up, is the model better?\n",
    "Not necessarily! We need to adjust it (r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c58d51-267b-4d21-ac07-a0abecdaf328",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Compare Model Fits - r-squared\n",
    "print(\"mod6res.rsquared: \", mod6res.rsquared)\n",
    "print(\"mod5res.rsquared: \", mod5res.rsquared)\n",
    "print(\"mod4res.rsquared: \", mod4res.rsquared)\n",
    "\n",
    "## Compare Model Fits - r-squared adjusted\n",
    "print(\"mod6res.rsquared_adj: \", mod6res.rsquared_adj)\n",
    "print(\"mod5res.rsquared_adj: \", mod5res.rsquared_adj)\n",
    "print(\"mod4res.rsquared_adj: \", mod4res.rsquared_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e1c14-9d87-4c7f-8e61-8ed224509112",
   "metadata": {},
   "source": [
    "Regression Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a2eca3-c5c3-4f3f-b31b-b18ad5091e37",
   "metadata": {},
   "source": [
    "#### Regression Diagnostics\n",
    "The Seaborn package allows us to visualize lowess curves and reveals potential non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a25291-4659-4fe6-bf10-3d19d121064f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Seaborn regression diagnostics: non-linear relationships\n",
    "sns.lmplot(x=\"disp\", y=\"mpg\", data=mtcars, lowess=True)\n",
    "mtcars.plot.scatter(x='mpg',y='disp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28d4f4-7ab7-446a-b56b-84aef480c223",
   "metadata": {},
   "source": [
    "The statsmodels graphics package has several plot methods that will assist with poor model fit identification. \n",
    "Review these on your screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf190b6-337b-4849-a7b2-a3502d4f58fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Examine the goodness of fit plots \n",
    "sm.graphics.plot_fit(mod5res,exog_idx=3)\n",
    "sm.graphics.plot_regress_exog(mod5res,exog_idx=3)\n",
    "sm.graphics.influence_plot(mod5res)\n",
    "sm.graphics.plot_leverage_resid2(mod5res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31efca5c-91d6-472e-82f8-f90bb7476923",
   "metadata": {},
   "source": [
    "### Scoring New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4976e39-702f-4bd3-ae9a-7a7892e17883",
   "metadata": {},
   "source": [
    "We are often interested in making predictions using a model, or “scoring” new data. \n",
    "We can easily do this with stats models\n",
    "We must first create a new dataset to score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96f1e5-4c47-455f-92f5-86d3a0ae463e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Scoring Data\n",
    "## create dataframe\n",
    "mtcars.cyl.unique()\n",
    "mtcars.disp.describe()\n",
    "new_dat = pd.DataFrame({'cyl':mtcars.cyl.unique(),\n",
    " 'disp':(80,100,120)})\n",
    "## Predict the new values\n",
    "mod5res.predict(new_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cbf642-d59c-4fa9-a693-3318d118d566",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c351a90e-fed3-4369-b17f-cd55ae6fcc54",
   "metadata": {},
   "source": [
    "Displacement appeared to have a non-linear relationship with mpg. Perhaps we should fit a quadratic term? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da7214f-389a-4fcf-832d-0100b3c429d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Linear regression with polynomials\n",
    "## add disp squared\n",
    "mtcars['disp_2'] = mtcars.disp **2\n",
    "mod7res = smf.ols('mpg ~ disp', data=mtcars).fit()\n",
    "mod7res.summary() \n",
    "mod8res = smf.ols('mpg ~ disp + disp_2', data=mtcars).fit()\n",
    "mod8res.summary() \n",
    "## Did this improve the model fit?\n",
    "sm.stats.anova_lm(mod8res, typ=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cba37a-047b-4e36-ad7b-1f27c44a0736",
   "metadata": {},
   "source": [
    "### Using Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbda18e-58b1-4407-b1f9-c33d6fa15230",
   "metadata": {},
   "source": [
    "We can use formulas when we know or have hypothesized the relationship. \n",
    "This has several advantages\n",
    "When we score data, it will automatically be transformed.\n",
    "No need to create intermediate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed6c09-1424-4ee9-b26c-3e2b4f6a2483",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## We can rewrite this with formulas...\n",
    "smf.ols('mpg ~ disp + disp_2', data=mtcars).fit().summary()\n",
    "smf.ols('mpg ~ disp + np.square(disp)', data=mtcars).fit().summary()\n",
    "smf.ols('mpg ~ disp + np.power(disp,2)', data=mtcars).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e2b5c5-5dd7-4478-9a4a-ea0989c52be5",
   "metadata": {},
   "source": [
    "## Other Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2480e2-5003-4349-a189-692815a8ff44",
   "metadata": {},
   "source": [
    "We can try any other vectorized functions using the formula interface as well. \n",
    "We may need to capture the non-linearity with expressions other than polynomials!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d55a077-352f-4e0d-a87a-0ffb69a1dc9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Linear regression with other functions (log)\n",
    "mod10res = smf.ols('mpg ~ np.log(disp)', data=mtcars).fit()\n",
    "mod10res.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ef8ef-9697-423d-abd0-c53e411ac067",
   "metadata": {},
   "source": [
    "### Patsy Formulas\n",
    "We mentioned that the Patsy package enables the formula interface.\n",
    "We can use Patsy alone to generate the X matrices, then pass them into the object\n",
    "**We must do this for machine learning** as there is no ML native support for formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b607f8-27fb-4745-a3e0-831c7474cbf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Using Patsy alone to generate matrices\n",
    "from patsy import dmatrices\n",
    "from patsy import dmatrix\n",
    "f = 'mpg ~ disp + C(cyl)'\n",
    "y,X = dmatrices(f, mtcars)\n",
    "y\n",
    "X\n",
    "y,X = dmatrices(f, mtcars,return_type='dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d662ab45-007e-4983-a4d8-de2c59db6376",
   "metadata": {},
   "source": [
    "We can now fit the model with our X and Y matrices\n",
    "Patsy has other useful functions including center and standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a97af-0c2d-4c9a-96aa-062a235ef07a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## notice we used OLS instead of ols...\n",
    "mod11res = smf.OLS(y,X).fit()\n",
    "mod11res.summary()\n",
    "\n",
    "### Other useful patsy functions\n",
    "## Center and standardize\n",
    "dmatrix(\"disp + center(disp) + standardize(disp)\", mtcars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f4c66-2391-4246-bde5-cf21ee66fa13",
   "metadata": {},
   "source": [
    "### Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd20cac-9813-48a7-8b6d-8a895f44d32f",
   "metadata": {},
   "source": [
    "We can also use Patsy to fit splines, or flexible fit curves to capture non-linearity.\n",
    "There is support for basis splines, cubic splines, and cyclic cubic splines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37990ead-d9f6-4bab-be4a-662730bbb8c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Linear regression with Spline terms\n",
    "## basis splines\n",
    "dmatrix(\"bs(disp,df=4)\", mtcars)\n",
    "\n",
    "## cubic splines\n",
    "dmatrix(\"cr(disp,df=4)\", mtcars)\n",
    "\n",
    "## cyclic cubic splines\n",
    "dmatrix(\"cc(disp,df=4)\", mtcars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee209de1-2d3a-48cb-8d03-3fb6f71ad166",
   "metadata": {},
   "source": [
    "## Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665eb7e-3901-484c-bc5b-b905551d2c04",
   "metadata": {},
   "source": [
    "What is an interaction in linear modeling?\n",
    "    It's when one variable depends upon another variable.\n",
    "    The dependency can be slight or great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589a4c1-352e-4d3d-9ee0-48e1fb831c51",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Linear regression with interactions\n",
    "## make interaction plot seaborn cyl, mpg, hp\n",
    "sns.lmplot(x=\"hp\", y=\"mpg\", hue=\"cyl\", data=mtcars);\n",
    "mod12 = smf.ols('mpg ~ hp + C(cyl)', \n",
    "        data=mtcars).fit()\n",
    "sm.stats.anova_lm(mod12, typ=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e896e04-0b6a-4a60-a6c7-e1606d365ffb",
   "metadata": {},
   "source": [
    "### Interactions in Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297c33b-893f-4c0c-bda3-89adcfdea957",
   "metadata": {},
   "source": [
    "We indicate interactions in models with the : (colon sign)\n",
    "We can also use  *  as a shortcut to include the interaction and the lower terms:\n",
    "x1*x2 -> expands to: x1 + x2 + x1:x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f8512b-307c-47a8-94ea-29743ae01009",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Test for an interaction here\n",
    "mod13 = smf.ols('mpg ~ hp + C(cyl) + hp:C(cyl)', \n",
    "        data=mtcars).fit()\n",
    "sm.stats.anova_lm(mod13, typ=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "798cc659-95ed-4f83-8348-ee4f3f2ccfcb",
   "metadata": {},
   "source": [
    "## Test your knowledge\n",
    "\n",
    "Understanding Heritability Through IQ Analysis\n",
    "\n",
    "Heritability is the degree to which genetic factors can explain variation in a trait within a population. It quantifies how well the characteristics of parents predict those of their offspring, on average, in the population. Narrow sense heritability is estimated as the slope of the regression line when plotting parent IQ (X) against offspring IQ (Y).\n",
    "\n",
    "![herit3.jpg](./images/herit3.jpg)\n",
    "\n",
    "Using the provided IQ dataset (.\\Datasets\\IQ.csv), complete the following:\n",
    "\n",
    "1. Calculate the narrow sense heritability of IQ by:\n",
    "- Using your Pandas skills to compute the mean parental IQ (average of both parents).\n",
    "- Performing a simple linear regression to evaluate the relationship between mean parental IQ (X) and offspring IQ (Y).\n",
    "\n",
    "2. Compare predictive models for child IQ:\n",
    "Test whether the mean parental IQ predicts offspring IQ better than using the individual IQs of both parents.\n",
    "- In addition to the regression above, perform both a single regression and a multiple regression (both parents' IQs predicting offspring IQ).\n",
    "- Use the Akaike Information Criterion (AIC) to determine which model provides the best fit.\n",
    "   \n",
    "3. Answer the quiz questions.ot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef6790-7b20-485b-87be-9cb61d9b4acf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "iq_qz=\"PythonQuizQuestions/iq_qz.json\"\n",
    "display_quiz(iq_qz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efebd37-8211-4eea-baf9-4037be95ef65",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "After this tutorial, you have seen how easily Python, with Pandas, stastmodel, and Seaborn libraries can perform statistical tests (ttest, ANOVA), find correlations & regressions (with all the parameters you need to know to find the best fit) and can generate the plots you need to understand the underlying relationships in your data. \n",
    "\n",
    "You are ready to carry out a **Guided exercise** using all of the tools from Submodule 2 with an [RNA seq dataset](./Submodule_2_Tutorial_5_PandasGuidedExercise.ipynb) before trying out a project.\n",
    "\n",
    "The next module, [Module 3](../Module_3/Submodule_3_Overview.ipynb), will lead you in hypothesis-free testing and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafdc1a7-c8f1-4d21-b033-fc20d81c84bc",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "Remember to shut down your Notebook compute instance when you are done for the day to avoid unnecessary charges. You can do this on the ml.azure.com toolbar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b57144-390e-4ad7-bff5-ec514ac42b75",
   "metadata": {},
   "source": [
    "##Solutions to Test your Knowledge questions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386fd054-24c2-4b71-831a-1aeeaae44ccb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "## Solution for ttest of protein expression quiz\n",
    "prot_exp_group = prot_exp.groupby(\"Group\")\n",
    "colors = {\"Control\": \"blue\", \"Treated\": \"orange\"}  # Define colors for groups\n",
    "for name, group in prot_exp_group:\n",
    "    plt.hist(group[\"Protein Expression (AU)\"], bins=7, alpha=0.5, label=name, color=colors[name])\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(\"Protein Expression (AU)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Protein Expression by Treatment Type\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "result= prot_exp.groupby(\"Group\")[\"Protein Expression (AU)\"].agg([\"mean\", \"std\"])\n",
    "print(result)\n",
    "sns.boxplot(x=\"Group\", y=\"Protein Expression (AU)\", data=prot_exp);\n",
    "sns.stripplot(x=\"Group\", y=\"Protein Expression (AU)\", data=prot_exp, color=\"red\", alpha=0.6, jitter=True);\n",
    "controlP=prot_exp[prot_exp[\"Group\"]==\"Control\"][\"Protein Expression (AU)\"]\n",
    "treatP=prot_exp[prot_exp[\"Group\"]==\"Treated\"][\"Protein Expression (AU)\"]\n",
    "# Perform independent two-sample t-test\n",
    "t_stat, p_value = ttest_ind(controlP, treatP)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb5dcb-31be-46e8-be72-6382dc499278",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cdf701-7dac-405b-9979-3ed1a749287f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#ANOVA for mut_rate.csv\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import os\n",
    "mutrate=pd.read_csv(\".\" +os.sep + \"Datasets\"+os.sep+ \"mut_rate.csv\")\n",
    "# Create an OLS model\n",
    "model = ols(\"Values  ~ Group\", data=mutrate).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "print(anova_table)\n",
    "#sns.boxplot(x=\"Group\", y=\"Values\", data=mutrate);\n",
    "#sns.stripplot(x=\"Group\", y=\"Values\", data=mutrate, color=\"orange\", alpha=0.6, jitter=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f226e2-4603-4859-8908-69bf92a3c1b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede79391-38f2-4b55-ac6f-ca2dca79d971",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Solution for the IQ quiz\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('.' + os.sep + \"Datasets\" + os.sep + \"IQ.csv\")\n",
    "\n",
    "# Display first few rows to inspect the data\n",
    "print(df.head())\n",
    "\n",
    "# 1. Calculate the mean parental IQ\n",
    "df['Mean_Parent_IQ'] = df[['fatheriq', 'motheriq']].mean(axis=1)\n",
    "\n",
    "# 2. Perform single regression (Mean Parental IQ -> Offspring IQ)\n",
    "model_single = smf.ols('childiq ~ Mean_Parent_IQ', data=df).fit()\n",
    "\n",
    "print(\"\\nSingle Regression (Mean Parental IQ -> Offspring IQ):\")\n",
    "print(model_single.summary())\n",
    "\n",
    "# 3. Perform multiple regression (Both Parent1 and Parent2 IQ -> Offspring IQ)\n",
    "model_multiple = smf.ols('childiq ~ fatheriq + motheriq', data=df).fit()  # Add intercept\n",
    "\n",
    "print(\"\\nMultiple Regression (Parent1 & Parent2 IQ -> Offspring IQ):\")\n",
    "print(model_multiple.summary())\n",
    "\n",
    "# 4. Compare models using AIC\n",
    "aic_single = model_single.aic\n",
    "aic_multiple = model_multiple.aic\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"Single Regression AIC: {aic_single}\")\n",
    "print(f\"Multiple Regression AIC: {aic_multiple}\")\n",
    "\n",
    "if aic_single < aic_multiple:\n",
    "    print(\"The single regression model (Mean Parental IQ) is the better fit based on AIC.\")\n",
    "else:\n",
    "    print(\"The multiple regression model (Parent1 and Parent2 IQ) is the better fit based on AIC.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
