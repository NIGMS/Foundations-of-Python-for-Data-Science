{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6718a8a-7828-41d7-88d1-3db8c6a2db37",
   "metadata": {},
   "source": [
    "# Tutorial 2: Pandas\n",
    "-----------------------------------------------\n",
    "## Overview\n",
    "NumPy is very fast at handling data. However, it is limited by allowing only ONE data type. Pandas adds the familiar data structure like a spreadsheet, with data organized in rows and columns with names. Why not just use Excel or Sheets, then? Because bioinformatics data sets, such as RNAseq results, are so large and you may need to do complex calculations on the data. Python tools are much faster, more powerful and more flexible than pre-packaged spreadsheet tools. Also, the data is typically not printed to the screen in a GUI interface so the processing speed is faster. If you process in the cloud, you can see substantial increases in speed of analysis.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "After this tutorial, you will be able to:\n",
    "- Define \"dataframe\"\n",
    "- Create a Pandas dataframe from scratch or from imported data\n",
    "- Work with column and rows\n",
    "- Sort, organize, query, and manipulate the Pandas dataframe\n",
    "- Visualize pandas dataframes with simple tools\n",
    "    \n",
    "### Prerequisites\n",
    "\n",
    "- Introductory Python\n",
    "- NumPy (tutorial 1)\n",
    "\n",
    "### Getting Started\n",
    "* One way to get introduced to Pandas is provided by pydata: [10 minutes to Pandas](https://pandas.pydata.org/docs/user_guide/10min.html) or their [getting started tutorials](https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html)\n",
    "* Watch the brief Lecture\n",
    "* As with NumPy, the custom is to import pandas with the alias of pd to save your typing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf443408-ada6-49b6-b823-bc1277f820b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this code box to install the required packages\n",
    "%pip install jupyterquiz\n",
    "import pandas as pd\n",
    "from jupyterquiz import display_quiz\n",
    "import os\n",
    "print(\"All needed tools imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c9bf1",
   "metadata": {},
   "source": [
    "## Why Pandas?​\n",
    "\n",
    "The scripting in Python is good till now, but what about data organization or handling columns of tabular data with different types? ​\n",
    "\n",
    "Pandas focuses on **data management** which can be combined with **analytics tools**​\n",
    "\n",
    "The core data type is a DataFrame. A DataFrame organizes data into rows and columns, making it easy to access, filter, and process.\n",
    "![Structure of a Dataframe](./images/pandasDF.png)\n",
    "\n",
    "You can think of it like an Excel spreadsheet or (more appropriately) like a database table​\n",
    "  - Tabular​\n",
    "  - Same data type within Column, but the data type can vary by column​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5acdc53-cbb2-42d8-8860-e7fb1e07ecb7",
   "metadata": {},
   "source": [
    "## Why use DataFrames in Bioinformatics?\n",
    "\n",
    "In bioinformatics, data often comes in tabular formats, such as gene expression matrices, genomic variant tables, or protein interaction datasets. A Pandas DataFrame provides:\n",
    "\n",
    "1. Ease of Handling Tabular Data:\n",
    "    - Rows can represent biological samples, sequences, or variants.\n",
    "    - Columns can represent genes, features, or metadata.\n",
    "    - Unlike Excel, Pandas does not attempt to display all the values all the time, so it is less demanding on computer memory-- especially for the large datasets common to bioinformatics\n",
    "\n",
    "2. Data Analysis: Perform operations like filtering, grouping, and summarizing efficiently.\n",
    "    - Example: Find the top 10 most expressed genes in RNA-Seq data.\n",
    "\n",
    "3. Integration: Read/write to biological data formats like CSV, Excel, or SQL.\n",
    "    - Example: Import genomic data from a CSV file and perform preprocessing.\n",
    "\n",
    "4. Visualization: Easily integrate with visualization libraries like Matplotlib\n",
    "    - Example: Plot expression levels of genes across conditions.\n",
    "    - Since Matplotlib can produce HIGH resolution images, this is a helpful tool for producing publication-ready figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0268ed6-32f1-4a31-a0a5-55e9edb10520",
   "metadata": {},
   "source": [
    "### Creating Data Frames\n",
    "\n",
    "A Pandas dataframe can be constructed in many ways. (see [pandas documentation](https://pandas.pydata.org/docs/user_guide/dsintro.html#dataframe) for more ways). Each column is called a **Series** and has it's own functions in Pandas [see more]\n",
    "We can create a dataframe by passing in a dictionary of equal length lists​\n",
    "\n",
    "  * The dictionary keys will be column names​\n",
    "\n",
    "We can also create dataframes from file loads and queries\n",
    "\n",
    "Here, you see how it is made with a dictionary as the data that you might assemble yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Dataframe\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "data = {'id': list(range(1,11)),\n",
    "'yr': list(range(2020,2025))*2,\n",
    "'count': np.random.randint(10,40,10)}\n",
    "frame = pd.DataFrame(data)\n",
    "\n",
    "frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cadc781-d3c7-4db5-8366-c211374d42d5",
   "metadata": {},
   "source": [
    "### Importing structured data sets (e.g. CSV)\n",
    "\n",
    "Probably the most common way to create a Pandas dataframe is to **import a CSV** for further analysis. ​\n",
    "\n",
    "You can also import excel, JSON, and the clipboard. [All the data types and import methods](http://pandas.pydata.org/pandas-docs/stable/io.html)\n",
    "\n",
    "Try the next box to import a portion of a large cancer dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4da3d-cf36-44cd-85d5-a865f3c86792",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = pd.read_csv(\"./Datasets/cancer.csv\")\n",
    "len(cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30455f-2e0b-4289-b565-04ade46782c7",
   "metadata": {},
   "source": [
    "### Another sample dataset: states\n",
    "Before we tackle bioinformatics datasets, it is customary to work with a toy dataset for which we have some intuition. A dataframe of information about the US states will allow you to learn and practice some Pandas skills.\n",
    "\n",
    "Can you import the states.csv file from the same folder as cancer? Assign that to the variable states. The code is commented out in the next section, in case you aren't able."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddc08e6-aa88-4e4c-9d34-4d10a51c84ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the states.csv file as a pandas dataframe as states\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd40a8",
   "metadata": {},
   "source": [
    "As soon as we import data, step one should be to check the data import​. Look at the first 3 rows. Showing 5 rows is default, or you can specify with df.head(n=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.read_csv(\".\" + os.sep + \"Datasets\" + os.sep + \"states.csv\")\n",
    "# View\n",
    "states.head(n=3)\n",
    "#states.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d79c32-1e8f-47bb-abcf-46a409cfe11c",
   "metadata": {},
   "source": [
    "## Pandas tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e185a-cb23-4dc2-b515-11ec3a228c5c",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "Are the number of rows as expected? What about the column names? General range of continuous variables? It's easy to asses​s (much more so than with a spreadsheet!!)\n",
    "\n",
    "A Pandas dataframe has a method (df.describe()) that can easily summarize each column. *The summary information is itself a dataframe*  To get summary statistics on a single column, just specify that with the name in quotes as shown.\n",
    "\n",
    "Can you obtain these same statistics for the cancer dataset? Note: these values could be meaningful for normalizing gene expression (later tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220afddc-e882-41ea-badd-cb5254c91d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "states.describe()\n",
    "#states[\"Murder\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375771da",
   "metadata": {},
   "source": [
    "### Rows and Columns \n",
    "\n",
    "We also may want to look at just the list of row identifiers (names), or column names. These are stored as attributes of the Pandas dataframe, so we can access the array directly as df.columns or df.index *Though unless reset, the rownames are just a range of numbers*  \n",
    "\n",
    "**What are the columns in the cancer dataset?**​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44802963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Columns\n",
    "states.columns\n",
    "# List the row identifiers (though it is just numbers, so python describes it succinctly!\n",
    "#states.index\n",
    "\n",
    "#you can change the indices\n",
    "#states.index=states[\"State\"]\n",
    "#states.index\n",
    "#states.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7be0b-4e8b-44c9-a153-fb9177b62469",
   "metadata": {},
   "source": [
    "As with np arrays, you can reference different dimensions.  **df.loc** attribute accesses a group of rows and columns by label(s) or a boolean array in the given DataFrame:\n",
    "df.loc [:, range] selects a set of columns\n",
    "df.loc[range,:] selects a set of rows​\n",
    "\n",
    "Thus, you can subset some rows and some columns based on columns. (next, we'll make a subset based on characteristics like size of the population.)\n",
    "\n",
    "The code is set up for the states dataframe. Try repeating for columns and/or rows in the cancer dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c5e00d-33f2-4e16-b3fa-c773bd46c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a slice of columns\n",
    "\n",
    "states.loc[0:8,'Population':'Illiteracy']\n",
    "#states.loc['Population':'Illiteracy']\n",
    "#states.loc[3:7,:]\n",
    "#states[3:7]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19191fa",
   "metadata": {},
   "source": [
    "### Indexes\n",
    "\n",
    "We can select portions of the data frame by indexes (rows and columns) in a variety of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c0322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing\n",
    "# Select Column. Head or the values are used to limit the display\n",
    "#states['State'].head()\n",
    "states.State[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d714bd1-e530-4573-9123-4e40a9c1cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or select more than one column\n",
    "states[['Income', 'State']].head()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Row (labels, index)\n",
    "states[0:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de521e",
   "metadata": {},
   "source": [
    "We can also index with **boolean​s**. We've shown how to select for states. \n",
    "\n",
    "Can you select for genes with a minimal level of expression across all the samples? across some of the samples? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94890cb7-8ea8-4ac0-8f49-55429a78a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select by boolean characteristics\n",
    "\n",
    "states[states.Murder <= 2.7]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d03a963-c313-4f67-93f0-452eaa91d6dd",
   "metadata": {},
   "source": [
    "### Test your Knowledge\n",
    "> 1. Efficiently determine how many states have a life expectancy between 70 and 71\n",
    "> 2. What is the average HS Graduate percentage for these states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33327e9-c444-43be-8e5d-5d214c31ded5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run this code box for a quiz\n",
    "from jupyterquiz import display_quiz\n",
    "lenqz= \"PythonQuizQuestions/pd_boolean_lengthQz.json\"\n",
    "display_quiz(lenqz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b82cb9-0cce-4383-b0d6-72ecddbb3bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answers\n",
    "x=states.loc[(states['Life Exp']<71) & (states['Life Exp']>70)]\n",
    "len(x)\n",
    "#x['HS Grad'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a06337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select by integer location\n",
    "states.iloc[0:5,0:3] \n",
    " \n",
    "# select columns by label, inclusive \n",
    "#states.loc[0:8,\"Population\":\"Murder\"]\n",
    "# end inclusive\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa93276-777c-4074-8dac-4cc97c654213",
   "metadata": {},
   "source": [
    "### Working with Text \n",
    "\n",
    "Strings in Pandas are roughly the same as strings in Python​\n",
    "\n",
    "We simply operate on series rather than a single object​\n",
    "\n",
    "Pandas provides many of the same methods, such as **len()**, **lower()**, **upper()**, **split()** and others you have seen before​\n",
    "\n",
    "Pandas methods also usually ignore missing NaN values​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4869c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(['Tom', 'William Rick', 'John', 'Alber@t'\n",
    "               , np.nan, '1234','SteveSmith'])\n",
    "\n",
    "print(s)\n",
    "print(s.str.len())\n",
    "#print(s.str.lower())\n",
    "#print(s.str.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f201e3-392f-4b76-8fa5-cd0c9d78b8da",
   "metadata": {},
   "source": [
    "## Practice your Pandas skills\n",
    "\n",
    "<p style=\"background:green;color:white;font-family:arial\">\n",
    "    Time to practice what you've just learned. You will:</p>\n",
    "\n",
    "1. Assemble the qPCR data into a dataframe\n",
    "2. Use indexing to access all Ct values for Sample1.\n",
    "3. Generate summary statistics for the Ct_Value column.\n",
    "4. Answer questions about the data by coding queries on the DataFrame.\n",
    "\n",
    "\n",
    "Here is your data and metadata (you'll have to edit slightly & use Pandas coding to make it into a dataframe.)\n",
    "- Target: \\['GeneA', 'GeneB', 'GeneC', 'GeneA', 'GeneB', 'GeneC'],\n",
    "- Sample: \\['Sample1', 'Sample1', 'Sample1', 'Sample2', 'Sample2', 'Sample2'],\n",
    "- Ct_Value: \\[25.4, 28.7, 30.1, 26.2, 29.5, 31.0]\n",
    "\n",
    "<p style=\"background:green;color:white;font-family:arial\">Use the next box for practice, then run the subsequent quiz python box. Solution options follow the quiz</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab8c9e-55f1-4ad1-b785-9605dc56f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a qPCR DataFrame\n",
    "\n",
    "#Access the Ct values and/or obtain summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2657d8-b4e9-4111-858b-c2f649144d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "indqz = \"PythonQuizQuestions/indqz.json\"\n",
    "display_quiz(indqz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b934a0f-a98f-4b7b-9a9a-2c56969a9ea4",
   "metadata": {},
   "source": [
    "How to solve the Knowledge Test problems\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Task</th>\n",
    "            <th>Required Coding</th>\n",
    "            <th>Answer</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Mean Ct value</td>\n",
    "            <td><code>df.describe()</code> or <code>df['Ct_Value'].mean()</code></td>\n",
    "            <td><strong>28.48</strong></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>All Ct for Sample1</td>\n",
    "            <td><code>df.loc[df['Sample'] == 'Sample1', 'Ct_Value']</code></td>\n",
    "            <td>Outputs all Sample1 Ct values</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Minimum Ct for GeneA</td>\n",
    "            <td><code>df.loc[df['Target'] == 'GeneA', 'Ct_Value'].min()</code></td>\n",
    "            <td><strong>25.4</strong></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed9d283-6136-4ebf-8a2f-f985a7139200",
   "metadata": {},
   "source": [
    "## Manipulating Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3025afa",
   "metadata": {},
   "source": [
    "### Adding/Deleting columns\n",
    "\n",
    "We can add columns by naming a new column, then assigning a set of values​. If the added column doesn't have enough values, Pandas will automatically add NaN\n",
    "\n",
    "Use the **del** keyword to delete a column​\n",
    "\n",
    "<p style=\"background:green;color:white;font-family:arial\"> Try this: You might have noticed that the first two columns of the cancer dataset are basically the same. Drop one of them & check the head to show you were successful.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e9248-ad0e-4e54-9eec-782bf264d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Column\n",
    "states[\"NewCol\"] = 4\n",
    " \n",
    "# Delete Column\n",
    "#del states[\"NewCol\"]\n",
    "states.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e1af2",
   "metadata": {},
   "source": [
    "### Adding/deleting ROWS\n",
    "​\n",
    "The syntax is slightly more complex to add a row to a Pandas dataframe. \n",
    "\n",
    "You can make a new row, then **concat**, that is to concatenate the data. This row should be a single dictionary for one row, or multiple dictionaries to add multiple rows. Since it's a dictionary, you don't have to put the data in the 'correct' order of the existing dataframe. \n",
    "\n",
    "new_row= pd.DataFrame(\\[{key:value, key1:value1, key2:value2}]\n",
    "\n",
    "If you do not give a value for each of the columns, then Pandas will use NaN\n",
    "\n",
    "Then, add that row of new data by concatenation. \n",
    "df = pd.concat(\\[df, new_rows], ignore_index=True)\n",
    "\n",
    "**To delete**, and to directly replace the dataframe using the inplace tag:\n",
    "\n",
    "df.drop(index=\\[Rowindex1, Rowindex2], inplace=True)\n",
    "\n",
    "If you just drop one row, use index=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2afe1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one or more Rows by Concatenation\n",
    "\n",
    "new_states = pd.DataFrame([{'State':'Puerto Rico', 'Murder':'17.2', 'Population':'3242'},{'State':'US VI', 'Population':'0.41','Murder':'40', 'HS Grad':'78'}])\n",
    "states = pd.concat([states, new_states], ignore_index=True)\n",
    "\n",
    "states.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d6cb0a-d9b8-4293-a29f-4a9257528b1e",
   "metadata": {},
   "source": [
    "Now, remove Puerto Rico row, with df.drop, using its index & check the tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f812e62-81c8-435f-9c2e-c72fa9ad3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "states.drop(index=50, inplace=True)\n",
    "states.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc997cda",
   "metadata": {},
   "source": [
    "### Reindexing After Subsetting\n",
    "Did you notice that while Pandas automatically adds the correct row index when concatenating, it does NOT automatically renumber or 'reindex'? \n",
    "\n",
    "You can use that to your advantage (remembering that US VI will always be index 51) or we can reindex:\n",
    "\n",
    "**df.reset_index(drop=True, inplace=True)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex\n",
    "states.reset_index(drop=True, inplace=True)  # you are dropping the column index which Pandas creates to \"remember\" the original index\n",
    "\n",
    "states.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83893b9f-ebdc-4173-b2ae-da38fad304f3",
   "metadata": {},
   "source": [
    "### Merge\n",
    "\n",
    "It might be useful to merge two dataframes together.  You need to picture them & ask yourself how they should be arranged- which of the 2 on the left & which on the right. *If it is just more ROWS with the same column names, that is accomplished as if you are \"adding rows\" as done above with df.concat* \n",
    "\n",
    "To merge them together, we can use the **pd.merge()** function & specify \"where\" each one goes. See the syntax below. Pandas uses information in your 2 dataframes to sync them (here, we specify that the STATE column is by what they are organized.\n",
    "\n",
    "pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None)\r\n",
    "see [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) for explanations:erge key).\r\n",
    "\n",
    "\n",
    "*The code below adds these two new columns to the right side of the sta and organizes based on statetes dataframe, but you should try adding to the left.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b44421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Import Dataframe\n",
    "#states = pd.read_csv(\".\" + os.sep + \"Datasets\" + \"states.csv\") # if you'd like a clean copy without USVI\n",
    "states2 = pd.read_csv(\".\" + os.sep + \"Datasets\" + os.sep + \"states_extra.csv\")\n",
    "states2.head()\n",
    "\n",
    "# Merge these together\n",
    "#states_merge=pd.merge(states,states2,how=\"left\", left_on=\"State\", right_on=\"State\")\n",
    "#states_merge.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac18a37",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------\n",
    "## Data wrangling\n",
    "\n",
    "A large challenge when working with real biological datasets is data \"cleaning.\" This removes values or samples with errors (negative read counts, participants over age 110) or missing data (NaN) that could skew downstream analysis. This process must be carried out systematically and carefully. The normal workflow for this is beyond this introductory module. Please look at the NIGMS Sandbox Machine Learning Module for a full treatment on this topic.\n",
    "\n",
    "### Dealing with Missing Values \n",
    "\n",
    "Pandas uses NaN to cover missing values​. You saw that when we added rows that did not give all the key:value pairs for every column.\n",
    "\n",
    "This may occur for many reasons in bioinformatics:\n",
    "1. A log2 fold-change might be NaN if the raw counts for a gene are zero in both conditions\n",
    "2. In multiple sequence alignments (MSAs), some positions may lack data for certain sequences due to indels or incomplete sequence coverage. After processing such data into a DataFrame, these gaps may appear as NaN.\n",
    "3. Combining datasets with mismatched identifiers or incomplete overlap of genes, samples, or annotations can introduce NaN values. For instance, merging a gene annotation file with an expression matrix may leave gaps for genes without annotations.\n",
    "4. RNA-seq data may lack expression levels for some genes in certain samples due to low expression or poor sequencing depth.\n",
    "\n",
    "You can drop a row (or col) that has missing data with **df.dropna(inplace=True)**    \n",
    "\n",
    "You can fill missing values in with **df.fillna(#, inplace=True)**\n",
    "\n",
    "- Why? for example for count data (e.g., RNA-seq), NaN might mean **no reads** were mapped, so replacing with *0* makes sense.\n",
    "\n",
    "This will depend on your **knowledge** of the system and what NaN means or does not implyfor your particular dataset.\n",
    "\n",
    "We will use a new function: mask(): Replace values where the condition is True.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Tip:</b> In the following code, you'll need to remove various # to see the different results. </a>. </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a80e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a dataset with missing Data\n",
    "states_miss  = states.copy() #so we can work with a separate copy, though states may incude USVI and/or PR\n",
    "\n",
    "# Set all Population values below 1000 to missing (NaN)\n",
    "states_miss['Population'] = states_miss['Population'].mask(states_miss['Population'] < 1000, np.nan)\n",
    "\n",
    "# Now, drop missing data\n",
    "states_miss.dropna(inplace=True)                       #drops any row that has NaN \n",
    "\n",
    "#Or, replace them with zeros\n",
    "#states_miss.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "#to drop the column with NaN in it\n",
    "#states_miss.dropna(axis=1, inplace=True)               # The only column with NaN is Population, which will be dropped from the states_miss dataframe\n",
    "#states_miss.head()\n",
    "\n",
    "#or, to change <1000 to \"low\" rather than NaN\n",
    "#states_miss['Population'] = states_miss['Population'].mask(states_miss['Population'] < 1000, \"low\")\n",
    "\n",
    "states_miss.loc[0:10,[\"State\",\"Population\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de25bb65",
   "metadata": {},
   "source": [
    "We use **.isnull()** and **.notnull()** to check for missing values, with a resulting dataframe of True and False values​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb213c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isnull\n",
    "states_miss.isnull().head()\n",
    "#states_miss.notnull()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e46916-0c7b-43cc-bee1-b881893e8c31",
   "metadata": {},
   "source": [
    "### Removing Duplicates\n",
    "\n",
    "You've already seen how to remove NaN. It is also straightforward to remove duplicated data.\n",
    "\n",
    "Duplicates in a Pandas DataFrame can be problematic for several reasons, depending on the context of your analysis or data manipulation.\n",
    "\n",
    "It might mean\n",
    "1. Data Integrity Issues\n",
    "2. Increased Storage and Processing\n",
    "3. Impact on Data Validation\n",
    "4. Issues with Data Joins or Merges\n",
    "5. Reduced Data Quality\n",
    "6. Challenges in Machine Learning\n",
    "\n",
    "Fortunately, Pandas has a simple way to manage these: \n",
    "\n",
    "- df.duplicated(): <u>Identifies</u> duplicate rows.\n",
    "- df.drop_duplicates(): <u>Removes</u> duplicate rows.</u>\n",
    "- df.duplicated(subset=\\[...]): Checks for duplicates in <u>specific columns</u> only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d5ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe with a duplicate row\n",
    "#states = pd.read_csv(\".\" + os.sep +\"Datasets\" + os.sep + \"states.csv\") # if you'd like a clean copy without USVI\n",
    "\n",
    "states_dup = pd.concat([states, states.iloc[[1]]], ignore_index=True)\n",
    "\n",
    "states_dup.tail()\n",
    "#Then, remove the duplicates\n",
    "#states_dup.duplicated().tail()  # Shows the boolean answer to 'is this a duplicate of another row?'\n",
    "\n",
    "#states_dup.drop_duplicates().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27737f4f",
   "metadata": {},
   "source": [
    "## Running functions on dataframes\n",
    "\n",
    "Remember that an advantage of using Pandas dataframes can streamline operations because they are built on top of NumPy, leveraging its efficient array processing capabilities, and they incorporate metadata (row/column labels, data types) that allow automatic alignment, indexing, and optimized handling of structured tabular data.\n",
    "\n",
    "Functions can operate in two basic ways on a dataframe. \n",
    "\n",
    "- Data Reduction: Functions which reduce a list of inputs to 1 output (e.g., the results in the 'describe' function like mean and max).\n",
    "     - These, by default, operate on the **column** ​\n",
    "- \"Element-wise\": Functions which return one output result for each input operate on the dataframe element-by-element.\n",
    "     -  df.applymap(lambda x: x\\**2)    *to divide ALL values in df by 2*\n",
    "     -  specify a column df\\[column].apply(lambda x: x+2) *to add 2 to specific columns*\n",
    "     -  round elements with df.round(2)​\n",
    " \n",
    "We'll recall our first dataframe above (called frame) which has only integers to demonstrate a few functions on the whole dataframe\n",
    "\n",
    "Note: lambda is a way to skip defining a function when we just need to do something function-like quickly\n",
    "\n",
    "The subsequent boxes illustrate mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c56970-4115-42f9-b179-c8a4387931be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions on DataFrames\n",
    "frame.mean()\n",
    "#cancer[\"2\"].mean()\n",
    "#result=states[\"Population\"].apply(lambda x: x/1000)\n",
    "\n",
    "#result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71303b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More Functions on DataFrames\n",
    " \n",
    "# Returning More than One Value\n",
    "states_copy = states.copy()\n",
    "states_copy[[\"Population\", \"Illiteracy\"]]=states_copy[[\"Population\", \"Illiteracy\"]].diff()\n",
    "states_copy.head()         #since there was nothing to subtract the first row of population & illiteracy frome, theose "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea8b08",
   "metadata": {},
   "source": [
    "Doing an element-wise operation is the same as our original notion of vectorization​\n",
    "\n",
    "Note in the sample here we are combining a table-wise and element-wise operation as a comparison​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e7be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3'])\n",
    "print(df.apply(np.mean))    # old mean\n",
    "print(df.applymap(lambda x:x*100)) # modify elements\n",
    "print(df.applymap(lambda x:x*100).apply(np.mean)) # new mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cd2a47",
   "metadata": {},
   "source": [
    "### Sorting a DF \n",
    "\n",
    "We use **.sort_values(by='A')** to sort a data frame by the values in column A (where A could be any column name. The default is to sort from low to high, but you can choose descending with ascending=False\n",
    "\n",
    "We can sort by more than one column(s), with ascending=\\[forFirstColumn?,forTheSecond?]​\n",
    "\n",
    "*we use .head() here only to limit the display size*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting\n",
    "states.sort_values(by='Population').head()\n",
    "#states.sort_values(by=\"Population\", ascending=False).head()\n",
    "\n",
    "# Multiple sorts\n",
    "#states.sort_values(by=[\"Illiteracy\",\"Population\"]).head()\n",
    "#states.sort_values(by=[\"Illiteracy\",\"Population\"], ascending=[True,False]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c14e7c",
   "metadata": {},
   "source": [
    "### Iterating over a series\n",
    "\n",
    "Iterating over a series (1d) object is similar to an array or tuple​\n",
    "\n",
    "Iterating over a dataframe or panel is a little trickier​\n",
    "\n",
    "The iteration for them provides a dictionary object which has the columns/values of rows/values​\n",
    "\n",
    "  - Use items() for column-wise​\n",
    "\n",
    "  - Use iterrows() for row-wise​\n",
    "  \n",
    "The **iteritems()** method produces key, value pairs with the column name and the column values as a series​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e29bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "\n",
    "for key, value in df.items():\n",
    "    print(key,\"   \", value)\n",
    "\n",
    "#for key, value in df.iterrows():\n",
    "#    print(key,\"   \", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8593c1d2",
   "metadata": {},
   "source": [
    "### Unique Values\n",
    "\n",
    "We can use the .unique() function of Series to Get unique values from a column​\n",
    "\n",
    "Use the sorted function to sort the output from unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Values\n",
    "states[\"Illiteracy\"].unique()\n",
    "\n",
    "# Unique Values from data set\n",
    "states.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39641de1-2784-435b-b09f-bc9d2a1f5705",
   "metadata": {},
   "source": [
    "## Test your knowledge\n",
    "The next code box provides a gene expression dataframe, but it has some duplicate data. Your tasks are to:\n",
    "\n",
    "1. **Remove Duplicate Rows:** Use Pandas to remove duplicate rows from the dataset based on all columns.\n",
    "2. **Sort by Expression Levels:** Sort the DataFrame by the Expression column in descending order to identify the highest expressed genes.\n",
    "3. **Answer the quiz questions**\n",
    "\n",
    "The solutions are provided after the quiz, if you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2d07f-5cb8-409e-b9b2-31ae9fe9b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Original DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Gene': ['GeneA', 'GeneB', 'GeneA', 'GeneC', 'GeneB', 'GeneD'],\n",
    "    'Sample': ['Sample1', 'Sample2', 'Sample1', 'Sample3', 'Sample2', 'Sample4'],\n",
    "    'Expression': [10.5, 20.2, 10.5, 15.3, 20.2, 12.1]\n",
    "})\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "#Remove duplicate rows\n",
    "\n",
    "#Sort by expression levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541fb5b-3232-4583-831e-bd6b5a6eec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "wrangle=\"PythonQuizQuestions/datawranglingQZ.json\"\n",
    "display_quiz(wrangle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521a0b5-0801-425c-aa87-7e48975479a7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Solutions to Test Your Knowedge Pandas Data Wrangling\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_unique = df.drop_duplicates()\n",
    "print(\"\\nDataFrame after removing duplicates:\")\n",
    "print(df_unique)\n",
    "\n",
    "# Answer to Question 1:\n",
    "unique_rows_count = len(df_unique)\n",
    "print(\"\\nNumber of unique rows:\", unique_rows_count)\n",
    "\n",
    "# Sort DataFrame by Expression in descending order\n",
    "df_sorted = df_unique.sort_values(by='Expression', ascending=False)\n",
    "print(\"\\nDataFrame sorted by Expression (descending):\")\n",
    "print(df_sorted)\n",
    "\n",
    "# Answer to Question 2:\n",
    "highest_expression_gene = df_sorted.iloc[0]['Gene']\n",
    "print(\"\\nGene with the highest expression:\", highest_expression_gene)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228a43cf",
   "metadata": {},
   "source": [
    "## Reshaping \n",
    "\n",
    "You may need to reshape a DataFrame in Pandas for bioinformatics problems because biological data often comes in complex formats that require adjustments to perform specific analyses. For example:\n",
    "\n",
    "1. Gene Expression Analysis: Reshaping is needed to pivot wide-form data (genes as columns) into long-form (genes as rows) for statistical modeling. \n",
    "2. Sequence Data: Transposing or reorganizing DataFrames allows better alignment of sequences (e.g., rows for samples, columns for nucleotides or amino acids).\n",
    "3. Multi-dimensional Data: Reshaping helps process hierarchical datasets (e.g., patients, samples, and measurements) by summarizing or splitting relevant portions.\n",
    "4. Comparative Analysis: To calculate differences or correlations, specific subsets of data (like df\\[\\['A', 'B']].diff()) may need reshaping to focus on sequential or paired comparisons.\n",
    "\n",
    "Essentially, reshaping can ensure that data aligns with the structure required by bioinformatics algorithms.\n",
    "\n",
    "The easiest change is **transforming** from rows to columns: df.T\n",
    "The code box shows how to transform AND to make the column names the sample information rather than row count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposing a dataframe\n",
    "# Example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Sample': ['A', 'B', 'C'],\n",
    "    'Gene1': [5.1, 4.8, 6.2],\n",
    "    'Gene2': [7.3, 6.9, 8.0]\n",
    "})\n",
    "print(df)\n",
    "# Transpose the DataFrame\n",
    "\n",
    "#print( df.set_index('Sample').T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1abe02",
   "metadata": {},
   "source": [
    "#### Reshaping from Long to Wide \n",
    "\n",
    "Scenario: You have experimental data where each sample is measured under multiple conditions, the data is in a long format.\n",
    " Sample  Condition  Measurement\n",
    "0      A    Control          5.1\n",
    "1      A  Treatment          7.2\n",
    "2      B    Control          4.8\n",
    "3      B  Treatment          6.9\n",
    "4      C    Control          5.9\n",
    "5      C  Treatment          8.1\n",
    "\n",
    "We need to reshape the data so that each *condition* becomes a *column*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d86e25-e4d1-493d-967c-83ca096e6288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.DataFrame({'Sample': ['A', 'A', 'B', 'B', 'C', 'C'], 'Condition': ['Control', 'Treatment', 'Control', 'Treatment', 'Control', 'Treatment'],'Measurement': [5.1, 7.2, 4.8, 6.9, 5.9, 8.1]})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09376bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to wide format\n",
    "wide_df = data.pivot(index='Sample', columns='Condition', values='Measurement')\n",
    "wide_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04521fc8",
   "metadata": {},
   "source": [
    "#### Reshaping from Wide to Long\n",
    "\n",
    "If you start with data in wide format and need to convert it back to long format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7499ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = wide_df.reset_index().melt(id_vars='Sample', var_name='Condition', value_name='Measurement')\n",
    "long_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5186981",
   "metadata": {},
   "source": [
    "### Making categories \n",
    "\n",
    "It might be that you need to divide your data into groups. For example, if you have a patient data set with the exact values of their temperatures, but you really just need to know \"normal, fever, severe fever\" as categories. Here we can show it this, too, with our states data. ​\n",
    "\n",
    "We can use **df.cut()**\n",
    "\n",
    "You describe the bins (this process can be called \"binning\") and assign names to the categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b5d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning with cut\n",
    "states.Population.describe()\n",
    "bins = [0,1000,3000,5000,25000]\n",
    "pd.cut(states.Population, bins)\n",
    "\n",
    "# With Labels\n",
    "states['Population_cat']= pd.cut(states.Population, bins, labels=[\"small\",\"med\",\"large\",\"huge\"]) #adds a new column with these bins\n",
    "states.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9892700",
   "metadata": {},
   "source": [
    "We often want to apply functions to groups within our dataset. ​\n",
    "\n",
    "  - What is the mean value within group?​\n",
    "\n",
    "  - What is the variance within group?​\n",
    "\n",
    "  - Perform a linear regression within group, then get the slope estimates out.​\n",
    "\n",
    "The first step is to establish the groups (SPLIT)​\n",
    "\n",
    "In Pandas, we can use the **.groupby()** method for this Use states, and group on the variable we just created ​\n",
    "\n",
    "Note: \"Groupby\" makes a new groupby pandas object. The **agg** function in a Pandas groupby object is used to apply one or more aggregation functions to grouped data. It allows you to compute summary statistics like mean, sum, min, max, and more, across multiple columns or for specific groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a400c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Category' and calculate mean\n",
    "result = states.groupby('Population_cat', observed=True)['HS Grad'].mean()\n",
    "result\n",
    "\n",
    "# Group by 'Category' and calculate both mean and sum of income within the category\n",
    "result2 = states.groupby('Population_cat', observed=True)['Income'].agg(['mean', 'sum'])\n",
    "print(result2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1428dc",
   "metadata": {},
   "source": [
    "We can index the group-by with a dynamically created value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e39d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group By Dynamic Variables\n",
    "states.Illiteracy.groupby(states.Income < 4100).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94668ba1",
   "metadata": {},
   "source": [
    "# Pandas Visuals \n",
    "\n",
    "The next tutorial is all about python visuals (mainly using matplotlib), but because it is so important to data analytics, pandas provides visuals too​\n",
    "\n",
    "Pandas supports many types of plots​\n",
    "\n",
    "  - Line, bar, area, box, histogram and scatter plots among others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7db936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(10,4),\n",
    "        index=pd.date_range('1/1/2000',\n",
    "        periods=10), columns=list('ABCD'))\n",
    "\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c41409",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.rand(10,4),\n",
    "        columns=['a','b','c','d'])\n",
    "df.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef51fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked bar\n",
    "df.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62583ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal stacked bar\n",
    "df.plot.barh(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'a':np.random.randn(1000)+1\n",
    "                   ,'b':np.random.randn(1000),'c':\n",
    "np.random.randn(1000) - 1}, columns=['a', 'b', 'c'])\n",
    "# histogram\n",
    "df.plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2614059",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.rand(50, 4)\n",
    "    , columns=['a', 'b', 'c', 'd'])\n",
    "df.plot.scatter(x='c', y='b')\n",
    "df.plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6811166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(3 * np.random.rand(4)\n",
    "    , index=['a', 'b', 'c', 'd'], columns=['x'])\n",
    "df.plot.pie(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c22f09-c9cc-4a66-bdfb-aebf97a41483",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "You now have tools to do a LOT of manipulations of data frames with Pandas in Python. You are ready to work through a bioinformatics exercise. [Exploring ligand binding sites](./Submodule_2_Tutorial_2b_PDB_Pandas_Exercise.ipynb) in a pdb file (protein structure file) using pandas and biopandas\n",
    "\n",
    "If you do not want to practice at this point, the next tutorial picks up with how to [visualize your data](./Submodule_2_Tutorial_3_VisualizingData.ipynb) with graphs in matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5406337e-cb46-411f-a771-d5a10321e78a",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Remember to stop your Jupyter Notebook compute instance to avoid unnecessary charges.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
